
@inproceedings{zhang_segmentation-free_2017,
	title = {Segmentation-Free Printed Traditional Mongolian {OCR} Using Sequence to Sequence with Attention Model},
	volume = {01},
	url = {https://ieeexplore.ieee.org/document/8270032/?arnumber=8270032},
	doi = {10.1109/ICDAR.2017.101},
	abstract = {Mongolian Optical Character Recognition ({OCR}) systems are required for printed document digitization and Mongolian cultural resources utilization. Existing Mongolian {OCR} systems are based on segmentation. But, the Mongolian segmentation is more difficult than other languages. So, these methods are highly costly and error suffering. In this study, a segmentation-free based traditional Mongolian word recognition method is proposed. Specifically, we formalize the {OCR} task as a sequence to sequence mapping problem, in which the input Mongolian word image and the output textual string are treated as a sequence of image frames and a sequence of letters, respectively. A sequence to sequence with attention model is adopted to solve this problem. Experimental results on a dataset show the effectiveness of the proposed method.},
	eventtitle = {2017 14th {IAPR} International Conference on Document Analysis and Recognition ({ICDAR})},
	pages = {585--590},
	booktitle = {2017 14th {IAPR} International Conference on Document Analysis and Recognition ({ICDAR})},
	author = {Zhang, Hui and Wei, Hongxi and Bao, Feilong and Gao, Guanglai},
	urldate = {2024-09-23},
	date = {2017-11},
	note = {{ISSN}: 2379-2140},
	keywords = {Attention, Character recognition, Decoding, Image segmentation, {LSTM}, Mongolian, Optical character recognition ({OCR}), Optical character recognition software, Sequence to sequence, Task analysis, Training, Vocabulary},
	file = {IEEE Xplore Abstract Record:/home/mikey/Zotero/storage/CYBFHZ72/8270032.html:text/html;IEEE Xplore Full Text PDF:/home/mikey/Zotero/storage/JXZLE2MF/Zhang et al. - 2017 - Segmentation-Free Printed Traditional Mongolian OCR Using Sequence to Sequence with Attention Model.pdf:application/pdf},
}

@inproceedings{rawls_combining_2017,
	title = {Combining Convolutional Neural Networks and {LSTMs} for Segmentation-Free {OCR}},
	volume = {01},
	url = {https://ieeexplore.ieee.org/document/8269965/?arnumber=8269965},
	doi = {10.1109/ICDAR.2017.34},
	abstract = {We present a novel end-to-end trainable {OCR} system combining a {CNN} for feature extraction with 1-D {LSTMs} for sequence modeling. We present results on English and Arabic handwriting data, and on English machine print data, showing state-of-the-art performance. We believe that our method is simpler than existing 2D {LSTM} models, and will make it easier to use techniques borrowed from {CNN} research in computer vision to improve {OCR} performance.},
	eventtitle = {2017 14th {IAPR} International Conference on Document Analysis and Recognition ({ICDAR})},
	pages = {155--160},
	booktitle = {2017 14th {IAPR} International Conference on Document Analysis and Recognition ({ICDAR})},
	author = {Rawls, Stephen and Cao, Huaigu and Kumar, Senthil and Natarajan, Prem},
	urldate = {2024-09-25},
	date = {2017-11},
	note = {{ISSN}: 2379-2140},
	keywords = {Decoding, Optical character recognition software, Training, Computational modeling, Feature extraction, Handwriting recognition, Hidden Markov models},
	file = {IEEE Xplore Abstract Record:/home/mikey/Zotero/storage/RB24AYTA/8269965.html:text/html;IEEE Xplore Full Text PDF:/home/mikey/Zotero/storage/WERWVXYN/Rawls et al. - 2017 - Combining Convolutional Neural Networks and LSTMs for Segmentation-Free OCR.pdf:application/pdf},
}

@inproceedings{etter_synthetic_2019,
	title = {A Synthetic Recipe for {OCR}},
	url = {https://ieeexplore.ieee.org/document/8978033},
	doi = {10.1109/ICDAR.2019.00143},
	abstract = {Synthetic data generation for optical character recognition ({OCR}) promises unlimited training data at zero annotation cost. With enough fonts and seed text, we should be able to generate data to train a model that approaches or exceeds the performance with real annotated data. Unfortunately, this is not always the reality. Unconstrained image settings, such as internet memes, scanned web pages, or newspapers, present diverse scripts, fonts, layouts, and complex backgrounds, which cause models trained with synthetic data to break down. In this work, we investigate the synthetic image generation problem on a large multilingual set of unconstrained document images. Our work presents a comprehensive evaluation of the impact of synthetic data attributes on model performance. The results provide a recipe for synthetic data generation that will help guide future research.},
	eventtitle = {2019 International Conference on Document Analysis and Recognition ({ICDAR})},
	pages = {864--869},
	booktitle = {2019 International Conference on Document Analysis and Recognition ({ICDAR})},
	author = {Etter, David and Rawls, Stephen and Carpenter, Cameron and Sell, Gregory},
	urldate = {2024-09-25},
	date = {2019-09},
	note = {{ISSN}: 2379-2140},
	keywords = {Optical character recognition software, Training, Data models, Image color analysis, Layout, Simultaneous localization and mapping, synthetic image generation, optical character recognition ({OCR}), unconstrained document images, multilingual, Training data},
	file = {IEEE Xplore Abstract Record:/home/mikey/Zotero/storage/2KGMW3JT/8978033.html:text/html;IEEE Xplore Full Text PDF:/home/mikey/Zotero/storage/PTZHB3RH/Etter et al. - 2019 - A Synthetic Recipe for OCR.pdf:application/pdf},
}

@software{belval_belvaltextrecognitiondatagenerator_2024,
	title = {Belval/{TextRecognitionDataGenerator}},
	rights = {{MIT}},
	url = {https://github.com/Belval/TextRecognitionDataGenerator},
	abstract = {A synthetic data generator for text recognition},
	author = {Belval, Edouard},
	urldate = {2024-09-25},
	date = {2024-09-25},
	keywords = {data, dataset, fake, ocr, synthetic, text, text-recognition, training-set-generator},
}

@inproceedings{rawls_combining_2017-1,
	title = {Combining deep learning and language modeling for segmentation-free {OCR} from raw pixels},
	url = {https://ieeexplore.ieee.org/document/8067772/?arnumber=8067772},
	doi = {10.1109/ASAR.2017.8067772},
	abstract = {We present a simple yet effective {LSTM}-based approach for recognizing machine-print text from raw pixels. We use a fully-connected feed-forward neural network for feature extraction over a sliding window, the output of which is directly fed into a stacked bi-directional {LSTM}. We train the network using the {CTC} objective function and use a {WFST} language model during recognition. Experimental results show that this simple system outperforms extensively tuned state-of-the-art {HMM} models on the {DARPA} Arabic Machine Print corpus.},
	eventtitle = {2017 1st International Workshop on Arabic Script Analysis and Recognition ({ASAR})},
	pages = {119--123},
	booktitle = {2017 1st International Workshop on Arabic Script Analysis and Recognition ({ASAR})},
	author = {Rawls, Stephen and Cao, Huaigu and Sabir, Ekraam and Natarajan, Prem},
	urldate = {2024-09-25},
	date = {2017-04},
	keywords = {Image segmentation, Optical character recognition software, Training, Feature extraction, Hidden Markov models, Data models, Neural networks},
	file = {IEEE Xplore Abstract Record:/home/mikey/Zotero/storage/4JKUBHXU/8067772.html:text/html;IEEE Xplore Full Text PDF:/home/mikey/Zotero/storage/W8WNQTGQ/Rawls et al. - 2017 - Combining deep learning and language modeling for segmentation-free OCR from raw pixels.pdf:application/pdf},
}

@software{noauthor_isi-vistavistaocr_2023,
	title = {isi-vista/{VistaOCR}},
	rights = {Apache-2.0},
	url = {https://github.com/isi-vista/VistaOCR},
	abstract = {{ISI}'s Optical Character Recognition ({OCR}) software for machine-print and handwriting data},
	publisher = {{ISI} Center for Vision, Image, Speech, and Text Analytics},
	urldate = {2024-09-25},
	date = {2023-02-25},
}

@inproceedings{ahmad_kpti_2016,
	title = {{KPTI}: Katib's Pashto Text Imagebase and Deep Learning Benchmark},
	url = {https://ieeexplore.ieee.org/document/7814106?denied=},
	doi = {10.1109/ICFHR.2016.0090},
	shorttitle = {{KPTI}},
	abstract = {This paper presents the first Pashto text image database for scientific research and thereby the first dataset with complete handwritten and printed text line images which ultimately covers all alphabets of Arabic and Persian languages. Language like Pashto, written in a complex way by calligraphers, still requires a mature Optical Character Recognition ({OCR}), system. Although 50 million people use this language both for oral and written communication, there is no significant effort which is devoted to the recognition of Pashto Script. A real dataset of 17,015 images having Pashto text lines is introduced. The images are acquired via scanning from hand scribed Pashto books. Further, in this work, we evaluated the performance of deep learning based models like Bidirectional and Multi-Dimensional Long Short Term Memory ({BLSTM} and {MDLSTM}) networks for Pashto texts and provide a baseline character error rate of 9.22\%.},
	eventtitle = {2016 15th International Conference on Frontiers in Handwriting Recognition ({ICFHR})},
	pages = {453--458},
	booktitle = {2016 15th International Conference on Frontiers in Handwriting Recognition ({ICFHR})},
	author = {Ahmad, Riaz and Afzal, M. Zeshan and Rashid, S. Faisal and Liwicki, Marcus and Breuel, Thomas and Dengel, Andreas},
	urldate = {2024-10-05},
	date = {2016-10},
	note = {{ISSN}: 2167-6445},
	keywords = {Character recognition, {LSTM}, Optical character recognition software, Handwriting recognition, Layout, Benchmark testing, Cursive script, {KPTI}, Machine learning, {OCR}, Pashto, Text recognition},
	file = {Full Text PDF:/home/mikey/Zotero/storage/XQ22VEIL/Ahmad et al. - 2016 - KPTI Katib's Pashto Text Imagebase and Deep Learning Benchmark.pdf:application/pdf},
}

@online{noauthor_rahmad77kpti_nodate,
	title = {rahmad77/{KPTI}: Katib's Pashto Text Imagebase},
	url = {https://github.com/rahmad77/KPTI/tree/master},
	urldate = {2024-10-05},
	file = {rahmad77/KPTI\: Katib's Pashto Text Imagebase:/home/mikey/Zotero/storage/PAS9HM2X/master.html:text/html},
}

@article{haq_nlpashto_2023,
	title = {{NLPashto}: {NLP} Toolkit for Low-resource Pashto Language},
	volume = {14},
	issn = {21565570, 2158107X},
	url = {http://thesai.org/Publications/ViewPaper?Volume=14&Issue=6&Code=IJACSA&SerialNo=142},
	doi = {10.14569/IJACSA.2023.01406142},
	shorttitle = {{NLPashto}},
	number = {6},
	journaltitle = {International Journal of Advanced Computer Science and Applications},
	shortjournal = {{IJACSA}},
	author = {Haq, Ijazul and Qiu, Weidong and Guo, Jie and Tang, Peng},
	urldate = {2024-10-05},
	date = {2023},
	langid = {english},
	file = {Full Text:/home/mikey/Zotero/storage/62QWDCTA/Haq et al. - 2023 - NLPashto NLP Toolkit for Low-resource Pashto Language.pdf:application/pdf},
}

@article{haq_correction_2023,
	title = {Correction of whitespace and word segmentation in noisy Pashto text using {CRF}},
	volume = {153},
	issn = {01676393},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639323001048},
	doi = {10.1016/j.specom.2023.102970},
	pages = {102970},
	journaltitle = {Speech Communication},
	shortjournal = {Speech Communication},
	author = {Haq, Ijazul and Qiu, Weidong and Guo, Jie and Tang, Peng},
	urldate = {2024-10-05},
	date = {2023-09},
	langid = {english},
}

@article{rowinski_namsel_2016,
	title = {Namsel: An Optical Character Recognition System for Tibetan Text},
	volume = {15},
	url = {https://escholarship.org/uc/item/6d5781k5},
	doi = {10.5070/H915129937},
	shorttitle = {Namsel},
	abstract = {The use of advanced computational methods for the analysis of large corpora of electronic texts is becoming increasingly popular in humanities and social science research. Unfortunately, Tibetan Studies has lacked such a repository of electronic, searchable texts. The automated recognition of printed texts, known as Optical Character Recognition ({OCR}), offers a solution to this problem; however, until recently, robust {OCR} systems for the Tibetan language have not been available. In this paper, we introduce one new system, called Namsel, which uses Optical Character Recognition ({OCR}) to support the production, review, and distribution of searchable Tibetan texts at a large scale. Namsel tackles a number of challenges unique to the recognition of complex scripts such as Tibean uchen and has been able to achieve high accuracy rates on a wide range of machine-printed works. In this paper, we discuss the details of Tibetan {OCR}, how Namsel works, and the problems it is able to solve. We also discuss the collaborative work between Namsel and its partner libraries aimed at building a comprehensive database of historical and modern Tibetan works—a database that consists of more than one million pages of texts spanning over a thousand years of literary production.},
	number = {1},
	journaltitle = {Himalayan Linguistics},
	author = {Rowinski, Zach and Keutzer, Kurt},
	urldate = {2024-10-05},
	date = {2016},
	langid = {english},
	file = {Full Text PDF:/home/mikey/Zotero/storage/BHUCEJ83/Rowinski and Keutzer - 2016 - Namsel An Optical Character Recognition System for Tibetan Text.pdf:application/pdf},
}

@online{noauthor_httpsaclanthologyorg2022findings-acl92pdf_nodate,
	title = {https://aclanthology.org/2022.findings-acl.92.pdf},
	url = {https://aclanthology.org/2022.findings-acl.92.pdf},
	urldate = {2024-10-24},
	file = {https\://aclanthology.org/2022.findings-acl.92.pdf:/home/mikey/Zotero/storage/993F27S7/2022.findings-acl.92.pdf:application/pdf},
}

@inproceedings{ignat_ocr_2022,
	location = {Dublin, Ireland},
	title = {{OCR} Improves Machine Translation for Low-Resource Languages},
	url = {https://aclanthology.org/2022.findings-acl.92},
	doi = {10.18653/v1/2022.findings-acl.92},
	abstract = {We aim to investigate the performance of current {OCR} systems on low resource languages and low resource scripts. We introduce and make publicly available a novel benchmark, {OCR}4MT, consisting of real and synthetic data, enriched with noise, for 60 low-resource languages in low resource scripts. We evaluate state-of-the-art {OCR} systems on our benchmark and analyse most common errors. We show that {OCR} monolingual data is a valuable resource that can increase performance of Machine Translation models, when used in backtranslation. We then perform an ablation study to investigate how {OCR} errors impact Machine Translation performance and determine what is the minimum level of {OCR} quality needed for the monolingual data to be useful for Machine Translation.},
	eventtitle = {Findings of the Association for Computational Linguistics: {ACL} 2022},
	pages = {1164--1174},
	booktitle = {Findings of the Association for Computational Linguistics: {ACL} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Ignat, Oana and Maillard, Jean and Chaudhary, Vishrav and Guzmán, Francisco},
	urldate = {2024-10-24},
	date = {2022},
	langid = {english},
	file = {PDF:/home/mikey/Zotero/storage/48MWHU6X/Ignat et al. - 2022 - OCR Improves Machine Translation for Low-Resource Languages.pdf:application/pdf},
}

@software{mmocr_contributors_openmmlab_2020,
	title = {{OpenMMLab} Text Detection, Recognition and Understanding Toolbox},
	rights = {Apache-2.0},
	url = {https://github.com/open-mmlab/mmocr},
	abstract = {{OpenMMLab} Text Detection, Recognition and Understanding Toolbox},
	version = {0.3.0},
	author = {{MMOCR Contributors}},
	urldate = {2024-10-25},
	date = {2020-08},
}

@misc{kuang_mmocr_2021,
	title = {{MMOCR}: A Comprehensive Toolbox for Text Detection, Recognition and Understanding},
	url = {http://arxiv.org/abs/2108.06543},
	doi = {10.48550/arXiv.2108.06543},
	shorttitle = {{MMOCR}},
	abstract = {We present {MMOCR}-an open-source toolbox which provides a comprehensive pipeline for text detection and recognition, as well as their downstream tasks such as named entity recognition and key information extraction. {MMOCR} implements 14 state-of-the-art algorithms, which is significantly more than all the existing open-source {OCR} projects we are aware of to date. To facilitate future research and industrial applications of text recognition-related problems, we also provide a large number of trained models and detailed benchmarks to give insights into the performance of text detection, recognition and understanding. {MMOCR} is publicly released at https://github.com/open-mmlab/mmocr.},
	number = {{arXiv}:2108.06543},
	publisher = {{arXiv}},
	author = {Kuang, Zhanghui and Sun, Hongbin and Li, Zhizhong and Yue, Xiaoyu and Lin, Tsui Hin and Chen, Jianyong and Wei, Huaqiang and Zhu, Yiqin and Gao, Tong and Zhang, Wenwei and Chen, Kai and Zhang, Wayne and Lin, Dahua},
	urldate = {2024-10-25},
	date = {2021-08-14},
	eprinttype = {arxiv},
	eprint = {2108.06543},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/mikey/Zotero/storage/LKTPGG23/Kuang et al. - 2021 - MMOCR A Comprehensive Toolbox for Text Detection, Recognition and Understanding.pdf:application/pdf;Snapshot:/home/mikey/Zotero/storage/Y67TJV8X/2108.html:text/html},
}

@misc{du_pp-ocr_2020,
	title = {{PP}-{OCR}: A Practical Ultra Lightweight {OCR} System},
	url = {http://arxiv.org/abs/2009.09941},
	doi = {10.48550/arXiv.2009.09941},
	shorttitle = {{PP}-{OCR}},
	abstract = {The Optical Character Recognition ({OCR}) systems have been widely used in various of application scenarios, such as office automation ({OA}) systems, factory automations, online educations, map productions etc. However, {OCR} is still a challenging task due to the various of text appearances and the demand of computational efficiency. In this paper, we propose a practical ultra lightweight {OCR} system, i.e., {PP}-{OCR}. The overall model size of the {PP}-{OCR} is only 3.5M for recognizing 6622 Chinese characters and 2.8M for recognizing 63 alphanumeric symbols, respectively. We introduce a bag of strategies to either enhance the model ability or reduce the model size. The corresponding ablation experiments with the real data are also provided. Meanwhile, several pre-trained models for the Chinese and English recognition are released, including a text detector (97K images are used), a direction classifier (600K images are used) as well as a text recognizer (17.9M images are used). Besides, the proposed {PP}-{OCR} are also verified in several other language recognition tasks, including French, Korean, Japanese and German. All of the above mentioned models are open-sourced and the codes are available in the {GitHub} repository, i.e., https://github.com/{PaddlePaddle}/{PaddleOCR}.},
	number = {{arXiv}:2009.09941},
	publisher = {{arXiv}},
	author = {Du, Yuning and Li, Chenxia and Guo, Ruoyu and Yin, Xiaoting and Liu, Weiwei and Zhou, Jun and Bai, Yifan and Yu, Zilin and Yang, Yehua and Dang, Qingqing and Wang, Haoshuang},
	urldate = {2024-10-25},
	date = {2020-10-15},
	eprinttype = {arxiv},
	eprint = {2009.09941},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/mikey/Zotero/storage/VSYED4J7/Du et al. - 2020 - PP-OCR A Practical Ultra Lightweight OCR System.pdf:application/pdf;Snapshot:/home/mikey/Zotero/storage/AH7EURGR/2009.html:text/html},
}

@inproceedings{smith_overview_2007,
	title = {An Overview of the Tesseract {OCR} Engine},
	volume = {2},
	url = {https://ieeexplore.ieee.org/document/4376991},
	doi = {10.1109/ICDAR.2007.4376991},
	abstract = {The Tesseract {OCR} engine, as was the {HP} Research Prototype in the {UNLV} Fourth Annual Test of {OCR} Accuracy, is described in a comprehensive overview. Emphasis is placed on aspects that are novel or at least unusual in an {OCR} engine, including in particular the line finding, features/classification methods, and the adaptive classifier.},
	eventtitle = {Ninth International Conference on Document Analysis and Recognition ({ICDAR} 2007)},
	pages = {629--633},
	booktitle = {Ninth International Conference on Document Analysis and Recognition ({ICDAR} 2007)},
	author = {Smith, R.},
	urldate = {2024-10-25},
	date = {2007-09},
	note = {{ISSN}: 2379-2140},
	keywords = {Optical character recognition software, Text recognition, Filters, Independent component analysis, Inspection, Open source software, Pipelines, Prototypes, Search engines, Testing},
	file = {IEEE Xplore Abstract Record:/home/mikey/Zotero/storage/3KC3F5HQ/4376991.html:text/html;Submitted Version:/home/mikey/Zotero/storage/BWK3ELA3/Smith - 2007 - An Overview of the Tesseract OCR Engine.pdf:application/pdf},
}

@online{noauthor_httpsaclanthologyorg2020lrec-1570pdf_nodate,
	title = {https://aclanthology.org/2020.lrec-1.570.pdf},
	url = {https://aclanthology.org/2020.lrec-1.570.pdf},
	urldate = {2024-10-25},
	file = {https\://aclanthology.org/2020.lrec-1.570.pdf:/home/mikey/Zotero/storage/ZLNJJE94/2020.lrec-1.570.pdf:application/pdf},
}

@misc{howard_searching_2019,
	title = {Searching for {MobileNetV}3},
	url = {http://arxiv.org/abs/1905.02244},
	doi = {10.48550/arXiv.1905.02244},
	abstract = {We present the next generation of {MobileNets} based on a combination of complementary search techniques as well as a novel architecture design. {MobileNetV}3 is tuned to mobile phone {CPUs} through a combination of hardware-aware network architecture search ({NAS}) complemented by the {NetAdapt} algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new {MobileNet} models for release: {MobileNetV}3-Large and {MobileNetV}3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling ({LR}-{ASPP}). We achieve new state of the art results for mobile classification, detection and segmentation. {MobileNetV}3-Large is 3.2{\textbackslash}\% more accurate on {ImageNet} classification while reducing latency by 15{\textbackslash}\% compared to {MobileNetV}2. {MobileNetV}3-Small is 4.6{\textbackslash}\% more accurate while reducing latency by 5{\textbackslash}\% compared to {MobileNetV}2. {MobileNetV}3-Large detection is 25{\textbackslash}\% faster at roughly the same accuracy as {MobileNetV}2 on {COCO} detection. {MobileNetV}3-Large {LR}-{ASPP} is 30{\textbackslash}\% faster than {MobileNetV}2 R-{ASPP} at similar accuracy for Cityscapes segmentation.},
	number = {{arXiv}:1905.02244},
	publisher = {{arXiv}},
	author = {Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and Le, Quoc V. and Adam, Hartwig},
	urldate = {2024-11-02},
	date = {2019-11-20},
	eprinttype = {arxiv},
	eprint = {1905.02244},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/mikey/Zotero/storage/B5TQXL4V/Howard et al. - 2019 - Searching for MobileNetV3.pdf:application/pdf;Snapshot:/home/mikey/Zotero/storage/5JVMNXEL/1905.html:text/html},
}

@inproceedings{graves_connectionist_2006,
	location = {New York, {NY}, {USA}},
	title = {Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks},
	isbn = {978-1-59593-383-6},
	url = {https://doi.org/10.1145/1143844.1143891},
	doi = {10.1145/1143844.1143891},
	series = {{ICML} '06},
	shorttitle = {Connectionist temporal classification},
	abstract = {Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks ({RNNs}) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training {RNNs} to label unsegmented sequences directly, thereby solving both problems. An experiment on the {TIMIT} speech corpus demonstrates its advantages over both a baseline {HMM} and a hybrid {HMM}-{RNN}.},
	pages = {369--376},
	booktitle = {Proceedings of the 23rd international conference on Machine learning},
	publisher = {Association for Computing Machinery},
	author = {Graves, Alex and Fernández, Santiago and Gomez, Faustino and Schmidhuber, Jürgen},
	urldate = {2024-11-02},
	date = {2006-06-25},
}

@misc{kingma_adam_2017,
	title = {Adam: A Method for Stochastic Optimization},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the infinity norm.},
	number = {{arXiv}:1412.6980},
	publisher = {{arXiv}},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	urldate = {2024-11-02},
	date = {2017-01-30},
	eprinttype = {arxiv},
	eprint = {1412.6980},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/home/mikey/Zotero/storage/XKETSSC8/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;Snapshot:/home/mikey/Zotero/storage/DNMUKLM9/1412.html:text/html},
}

@inproceedings{lawrie_building_2020,
	location = {Marseille, France},
	title = {Building {OCR}/{NER} Test Collections},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.570},
	abstract = {Named entity recognition ({NER}) identifies spans of text that contain names. Many researchers have reported the results of {NER} on text created through optical character recognition ({OCR}) over the past two decades. Unfortunately, the test collections that support this research are annotated with named entities after optical character recognition ({OCR}) has been run. This means that the collection must be re-annotated if the {OCR} output changes. Instead by tying annotations to character locations on the page, a collection can be built that supports {OCR} and {NER} research without requiring re-annotation when either improves. This means that named entities are annotated on the transcribed text. The transcribed text is all that is needed to evaluate the performance of {OCR}. For {NER} evaluation, the tagged {OCR} output is aligned to the transcriptions the aligned files, creating modified files of each, which are scored. This paper presents a methodology for building such a test collection and releases a collection of Chinese {OCR}-{NER} data constructed using the methodology. The paper provides performance baselines for current {OCR} and {NER} systems applied to this new collection.},
	eventtitle = {{LREC} 2020},
	pages = {4639--4646},
	booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
	publisher = {European Language Resources Association},
	author = {Lawrie, Dawn and Mayfield, James and Etter, David},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	urldate = {2024-11-02},
	date = {2020-05},
	file = {Full Text PDF:/home/mikey/Zotero/storage/3K7GVIYN/Lawrie et al. - 2020 - Building OCRNER Test Collections.pdf:application/pdf},
}

@software{zahidaz_zahidazpashto_fonts_2024,
	title = {zahidaz/pashto\_fonts},
	url = {https://github.com/zahidaz/pashto_fonts},
	abstract = {Collection of Pashto fonts},
	author = {zahidaz},
	urldate = {2024-11-02},
	date = {2024-08-04},
}

@inproceedings{he_deep_2016,
	title = {Deep Residual Learning for Image Recognition},
	url = {https://ieeexplore.ieee.org/document/7780459?denied=},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the {ImageNet} dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than {VGG} nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the {ImageNet} test set. This result won the 1st place on the {ILSVRC} 2015 classification task. We also present analysis on {CIFAR}-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the {COCO} object detection dataset. Deep residual nets are foundations of our submissions to {ILSVRC} \& {COCO} 2015 competitions1, where we also won the 1st places on the tasks of {ImageNet} detection, {ImageNet} localization, {COCO} detection, and {COCO} segmentation.},
	eventtitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {770--778},
	booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2024-11-02},
	date = {2016-06},
	note = {{ISSN}: 1063-6919},
	keywords = {Complexity theory, Degradation, Image recognition, Image segmentation, Neural networks, Training, Visualization},
	file = {Accepted Version:/home/mikey/Zotero/storage/AJGNZJC7/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf;IEEE Xplore Abstract Record:/home/mikey/Zotero/storage/53LCDMYM/7780459.html:text/html},
}

@misc{howard_mobilenets_2017,
	title = {{MobileNets}: Efficient Convolutional Neural Networks for Mobile Vision Applications},
	url = {http://arxiv.org/abs/1704.04861},
	doi = {10.48550/arXiv.1704.04861},
	shorttitle = {{MobileNets}},
	abstract = {We present a class of efficient models called {MobileNets} for mobile and embedded vision applications. {MobileNets} are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on {ImageNet} classification. We then demonstrate the effectiveness of {MobileNets} across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
	number = {{arXiv}:1704.04861},
	publisher = {{arXiv}},
	author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
	urldate = {2024-11-02},
	date = {2017-04-17},
	eprinttype = {arxiv},
	eprint = {1704.04861},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/mikey/Zotero/storage/FUE9CGNY/Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Networks for Mobile Vision Applications.pdf:application/pdf;Snapshot:/home/mikey/Zotero/storage/VBZJY8WB/1704.html:text/html},
}

@misc{magueresse_low-resource_2020,
	title = {Low-resource Languages: A Review of Past Work and Future Challenges},
	url = {http://arxiv.org/abs/2006.07264},
	doi = {10.48550/arXiv.2006.07264},
	shorttitle = {Low-resource Languages},
	abstract = {A current problem in {NLP} is massaging and processing low-resource languages which lack useful training attributes such as supervised data, number of native speakers or experts, etc. This review paper concisely summarizes previous groundbreaking achievements made towards resolving this problem, and analyzes potential improvements in the context of the overall future research direction.},
	number = {{arXiv}:2006.07264},
	publisher = {{arXiv}},
	author = {Magueresse, Alexandre and Carles, Vincent and Heetderks, Evan},
	urldate = {2024-11-02},
	date = {2020-06-12},
	eprinttype = {arxiv},
	eprint = {2006.07264},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/mikey/Zotero/storage/G9YRDPFS/Magueresse et al. - 2020 - Low-resource Languages A Review of Past Work and Future Challenges.pdf:application/pdf;Snapshot:/home/mikey/Zotero/storage/YZGETF3Q/2006.html:text/html},
}

@online{noauthor_pashto_nodate,
	title = {Pashto {\textbar} Silk Roads Programme},
	url = {https://en.unesco.org/silkroad/silk-road-themes/languages-and-endanger-languages/pashto},
	urldate = {2024-11-02},
    author = {UNESCO},
	file = {Pashto | Silk Roads Programme:/home/mikey/Zotero/storage/WNBMMLTG/pashto.html:text/html},
}
